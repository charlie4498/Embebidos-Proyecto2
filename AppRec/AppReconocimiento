
import cv2
import numpy as np
import sys
from timeit import default_timer as timer
import tflite_runtime.interpreter as tflite

# This function is for designing the overlay text on the predicted image boxes.
def text_on_detected_boxes(text,text_x,text_y,image,font_scale = 1,

                           font = cv2.FONT_HERSHEY_SIMPLEX,

                           FONT_COLOR = (0, 0, 0),

                           FONT_THICKNESS = 2,

                           rectangle_bgr = (0, 255, 0)):

    # get the width and height of the text box

    (text_width, text_height) = cv2.getTextSize(text, font, fontScale=font_scale, thickness=2)[0]

    # Set the Coordinates of the boxes

    box_coords = ((text_x-10, text_y+4), (text_x + text_width+10, text_y - text_height-5))

    # Draw the detected boxes and labels

    cv2.rectangle(image, box_coords[0], box_coords[1], rectangle_bgr, cv2.FILLED)

    cv2.putText(image, text, (text_x, text_y), font, fontScale=font_scale, color=FONT_COLOR,thickness=FONT_THICKNESS)

# Deteccion de cara
def face_detector_video(img):

    # Convert image to grayscale

    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

    faces = face_classifier.detectMultiScale(gray, 1.3, 5)

    if faces is ():

        return (0, 0, 0, 0), np.zeros((48, 48), np.uint8), img

    for (x, y, w, h) in faces:

        cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), thickness=2)

        roi_gray = gray[y:y + h, x:x + w]

    roi_gray = cv2.resize(roi_gray, (48, 48), interpolation=cv2.INTER_AREA)

    return (x, w, y, h), roi_gray, img

#Deteccion de emociones
def emotionVideo(cap):
    x=True
    tstart= timer()
    tref= timer()
    while x:

        ret, frame = cap.read()

        rect, face, image = face_detector_video(frame)

        if np.sum([face]) != 0.0:

            #roi = face.astype("float") / 255.0

            #roi1 = img_to_array(roi)
            roi = np.array(face, dtype= np.float32)

            roi = np.expand_dims(np.expand_dims(roi, axis=0), axis=-1)


            interpreter.set_tensor(input_details[0]['index'], roi)

            interpreter.invoke()

            preds = interpreter.get_tensor(output_details[0]['index'])

            # make a prediction on the ROI, then lookup the class

            #preds = classifier.predict(roi)[0]

            #label = class_labels[preds.argmax()]

            label= class_labels[int(np.argmax(preds))]

            if (timer()-tref >= Tm):
                open('Emociones.txt','a').write(label + '\n')
                tref= tref + Tm

            label_position = (rect[0] + rect[1]//50, rect[2] + rect[3]//50)

            text_on_detected_boxes(label, label_position[0], label_position[1], image) # You can use this function for your another opencv projects.

            fps = cap.get(cv2.CAP_PROP_FPS)

            cv2.putText(image, str(fps),(5, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

            if (timer()-tstart) >= T:
                x=False
        else:

            cv2.putText(image, "No Face Found", (5, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)

        cv2.imshow('All', image)

        if cv2.waitKey(1) & 0xFF == ord('q'):

            break

    cap.release()

    cv2.destroyAllWindows()


interpreter=tflite.Interpreter(model_path="converted_model.tflite")
interpreter.allocate_tensors()

# Get input and output tensors.
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# We have 6 labels for the model

class_labels = {0:'angry', 1:'disgust', 2:'fear', 3:'happy', 4:'sad', 5:'surprise', 6:'neutral'}
classes = list(class_labels.values())

# print(class_labels)

face_classifier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')

camera = cv2.VideoCapture(0) # If you are using an USB Camera then Change use 1 instead of 0.

T= int(sys.argv[1]) #Tiempo de ejecucion
Tm= int(sys.argv[2]) #Tiempo de Muestreo

f= open('Emociones.txt', 'r+')
f.truncate(0)
f.close

emotionVideo(camera)


